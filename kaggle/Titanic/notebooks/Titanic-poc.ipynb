{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T22:39:44.015641Z",
     "start_time": "2020-05-20T22:39:40.069585Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: apache-beam 2.22.0 has requirement httplib2<0.18.0,>=0.8, but you'll have httplib2 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: apache-beam 2.22.0 has requirement mock<3.0.0,>=1.0.1, but you'll have mock 4.0.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kaggle\n",
    "#!pip install google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'kaggle.json': No such file or directory\n",
      "kaggle.json\n",
      "-rw------- 1 jupyter jupyter 73 Jul 21 14:17 /home/jupyter/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!ls -l ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T21:02:56.075438Z",
     "start_time": "2020-05-17T21:02:55.972412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access './../data': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "data_base_path = os.path.join(os.path.curdir, '../data')\n",
    "titanic_base_path = os.path.join(data_base_path, 'titanic')\n",
    "\n",
    "!ls $data_base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T22:37:37.832561Z",
     "start_time": "2020-05-20T22:37:37.826565Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_kaggle(dataset=\"titanic\", data_base_path=data_base_path):\n",
    "    data_path = os.path.join(os.path.curdir, f\"{data_base_path}/{dataset}\")\n",
    "    !kaggle competitions download -c $dataset --path $data_path --force\n",
    "    !unzip -o $data_path/titanic.zip -d $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T22:38:01.443871Z",
     "start_time": "2020-05-20T22:38:00.330969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading titanic.zip to ././../data/titanic\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 20.7MB/s]\n",
      "Archive:  ././../data/titanic/titanic.zip\n",
      "  inflating: ././../data/titanic/gender_submission.csv  \n",
      "  inflating: ././../data/titanic/test.csv  \n",
      "  inflating: ././../data/titanic/train.csv  \n"
     ]
    }
   ],
   "source": [
    "load_data_from_kaggle(dataset=\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T23:48:52.908421Z",
     "start_time": "2020-05-23T23:48:52.904397Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_titanic_dateset(titanic_path=titanic_base_path):\n",
    "    gender_submission_csv_path = os.path.join(titanic_path, \"gender_submission.csv\")\n",
    "    train_csv_path = os.path.join(titanic_path, \"train.csv\")\n",
    "    test_csv_path = os.path.join(titanic_path, \"test.csv\")\n",
    "    return pd.read_csv(gender_submission_csv_path), pd.read_csv(train_csv_path), pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T23:48:55.753459Z",
     "start_time": "2020-05-23T23:48:55.702462Z"
    }
   },
   "outputs": [],
   "source": [
    "gender_sub_df, train_df, test_df = load_titanic_dateset(titanic_path=titanic_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T23:52:16.597837Z",
     "start_time": "2020-05-23T23:52:16.578833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = None\n",
    "most_embarked_from = None\n",
    "train_fare_mean = None\n",
    "train_fare_std = None\n",
    "\n",
    "def drop_columns(df):\n",
    "    df.drop(\"Name\", axis=1, inplace=True)\n",
    "    df.drop(\"PassengerId\", axis=1, inplace=True)\n",
    "    df.drop(\"Ticket\", axis=1, inplace=True)\n",
    "    df.drop(\"Cabin\", axis=1, inplace=True)\n",
    "    df.drop(\"Sex_female\", axis=1, inplace=True)\n",
    "    df.drop('SibSp', axis=1, inplace=True)\n",
    "    df.drop('Parch', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    global median_age\n",
    "    global most_embarked_from\n",
    "    global train_fare_mean\n",
    "    global train_fare_std\n",
    "    \n",
    "    median_age = median_age or copy_df[\"Age\"].median(skipna=True)\n",
    "    most_embarked_from = most_embarked_from or copy_df[\"Embarked\"].value_counts().idxmax()\n",
    "    train_fare_mean = train_fare_mean or copy_df[\"Fare\"].mean(skipna=True)\n",
    "    train_fare_std = train_fare_std or copy_df[\"Fare\"].std(skipna=True)\n",
    "    \n",
    "    copy_df[\"Age\"].fillna(median_age, inplace=True)\n",
    "    copy_df[\"Embarked\"].fillna(most_embarked_from, inplace=True)\n",
    "    copy_df[\"Fare\"] = (copy_df[\"Fare\"] - train_fare_mean) / train_fare_std\n",
    "    \n",
    "    return copy_df\n",
    "    \n",
    "def create_categorical(df):\n",
    "    copy_df = pd.get_dummies(df, columns=[\"Pclass\", \"Embarked\", \"Sex\"])\n",
    "    pd.bu\n",
    "    return copy_df\n",
    "\n",
    "def add_engineered(df):\n",
    "    df['TravelAlone'] = np.where((df[\"SibSp\"] + df[\"Parch\"])>0, 0, 1)\n",
    "    df['TravelAlone'] = df['TravelAlone'].astype('uint8')\n",
    "    return df\n",
    "\n",
    "def change_types(df):\n",
    "    df['TravelAlone'] = df['TravelAlone'].astype('uint8')\n",
    "    if 'Survived' in df.columns:\n",
    "        df['Survived'] = df['Survived'].astype('uint8')\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    copy_df = fill_missing_values(copy_df)\n",
    "    copy_df = create_categorical(copy_df)\n",
    "    copy_df = add_engineered(copy_df)\n",
    "    copy_df = drop_columns(copy_df)\n",
    "    copy_df = change_types(copy_df)\n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_clean = clean_data(train_df)\n",
    "test_df_clean = clean_data(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>TravelAlone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.502163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.786404</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.488580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.420494</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-0.486064</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived   Age      Fare  Pclass_1  Pclass_2  Pclass_3  Embarked_C  \\\n",
       "0         0  22.0 -0.502163         0         0         1           0   \n",
       "1         1  38.0  0.786404         1         0         0           1   \n",
       "2         1  26.0 -0.488580         0         0         1           0   \n",
       "3         1  35.0  0.420494         1         0         0           0   \n",
       "4         0  35.0 -0.486064         0         0         1           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Sex_male  TravelAlone  \n",
       "0           0           1         1            0  \n",
       "1           0           0         0            0  \n",
       "2           0           1         0            1  \n",
       "3           0           1         0            0  \n",
       "4           0           1         1            1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_clean.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Survived     891 non-null    uint8  \n",
      " 1   Age          891 non-null    float64\n",
      " 2   Fare         891 non-null    float64\n",
      " 3   Pclass_1     891 non-null    uint8  \n",
      " 4   Pclass_2     891 non-null    uint8  \n",
      " 5   Pclass_3     891 non-null    uint8  \n",
      " 6   Embarked_C   891 non-null    uint8  \n",
      " 7   Embarked_Q   891 non-null    uint8  \n",
      " 8   Embarked_S   891 non-null    uint8  \n",
      " 9   Sex_male     891 non-null    uint8  \n",
      " 10  TravelAlone  891 non-null    uint8  \n",
      "dtypes: float64(2), uint8(9)\n",
      "memory usage: 21.9 KB\n"
     ]
    }
   ],
   "source": [
    "train_df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Age', 'Fare', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Sex_male', 'TravelAlone'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featcols = {\n",
    "  colname : tf.feature_column.numeric_column(colname) \\\n",
    "    for colname in 'Age,Fare,Pclass_1,Pclass_2,Pclass_3,Embarked_C,Embarked_Q,Embarked_S,Sex_male,TravelAlone'.split(',')\n",
    "}\n",
    "featcols.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and eval\n",
    "msk = np.random.rand(len(train_df_clean)) < 0.8\n",
    "traindf = train_df_clean[msk]\n",
    "evaldf = train_df_clean[~msk]\n",
    "\n",
    "BATCH_SIZE= 20\n",
    "OUTDIR = '../models'\n",
    "\n",
    "def make_input_fn(df, mode, batch_size = BATCH_SIZE):\n",
    "    global mean_train_fare\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # loop indefinetly\n",
    "        shuffle=True\n",
    "    else:\n",
    "        num_epochs = 1 # one run and it's over\n",
    "        shuffle=False\n",
    "    \n",
    "    return tf.compat.v1.estimator.inputs.pandas_input_fn(x = df[list(featcols.keys())],\n",
    "                                                y = df[\"Survived\"],\n",
    "                                                num_epochs = num_epochs,\n",
    "                                                batch_size = batch_size, \n",
    "                                                shuffle = shuffle)\n",
    "\n",
    "def train_input_fn(df, batch_size=BATCH_SIZE):\n",
    "    return make_input_fn(df, mode=tf.estimator.ModeKeys.TRAIN, batch_size=batch_size)\n",
    "\n",
    "def eval_input_fn(df):\n",
    "    return make_input_fn(df, mode=tf.estimator.ModeKeys.EVAL, batch_size=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../models', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ../models/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.6172822, step = 0\n",
      "INFO:tensorflow:global_step/sec: 87.035\n",
      "INFO:tensorflow:loss = 0.666014, step = 100 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.087\n",
      "INFO:tensorflow:loss = 0.61622316, step = 200 (0.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.993\n",
      "INFO:tensorflow:loss = 0.6648936, step = 300 (0.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.053\n",
      "INFO:tensorflow:loss = 0.6152609, step = 400 (0.741 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.816\n",
      "INFO:tensorflow:loss = 0.6029297, step = 500 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 129.883\n",
      "INFO:tensorflow:loss = 0.5999605, step = 600 (0.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.846\n",
      "INFO:tensorflow:loss = 0.65812683, step = 700 (0.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.669\n",
      "INFO:tensorflow:loss = 0.556976, step = 800 (0.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.546\n",
      "INFO:tensorflow:loss = 0.61155164, step = 900 (0.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.845\n",
      "INFO:tensorflow:loss = 0.77947277, step = 1000 (0.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.119\n",
      "INFO:tensorflow:loss = 0.7024506, step = 1100 (0.757 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.214\n",
      "INFO:tensorflow:loss = 0.708471, step = 1200 (0.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.312\n",
      "INFO:tensorflow:loss = 0.61712813, step = 1300 (0.716 sec)\n",
      "INFO:tensorflow:global_step/sec: 150.269\n",
      "INFO:tensorflow:loss = 0.6463383, step = 1400 (0.660 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.454\n",
      "INFO:tensorflow:loss = 0.66368806, step = 1500 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 148.106\n",
      "INFO:tensorflow:loss = 0.623149, step = 1600 (0.675 sec)\n",
      "INFO:tensorflow:global_step/sec: 153.556\n",
      "INFO:tensorflow:loss = 0.6639279, step = 1700 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 130.818\n",
      "INFO:tensorflow:loss = 0.6983355, step = 1800 (0.760 sec)\n",
      "INFO:tensorflow:global_step/sec: 131.493\n",
      "INFO:tensorflow:loss = 0.58450097, step = 1900 (0.763 sec)\n",
      "INFO:tensorflow:global_step/sec: 139.446\n",
      "INFO:tensorflow:loss = 0.79554975, step = 2000 (0.714 sec)\n",
      "INFO:tensorflow:global_step/sec: 149.126\n",
      "INFO:tensorflow:loss = 0.63818425, step = 2100 (0.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 141.26\n",
      "INFO:tensorflow:loss = 0.5860367, step = 2200 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.943\n",
      "INFO:tensorflow:loss = 0.6331951, step = 2300 (0.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 146.32\n",
      "INFO:tensorflow:loss = 0.71224433, step = 2400 (0.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.925\n",
      "INFO:tensorflow:loss = 0.6705786, step = 2500 (0.797 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.147\n",
      "INFO:tensorflow:loss = 0.66998273, step = 2600 (0.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.93\n",
      "INFO:tensorflow:loss = 0.69709027, step = 2700 (0.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.242\n",
      "INFO:tensorflow:loss = 0.6165376, step = 2800 (0.744 sec)\n",
      "INFO:tensorflow:global_step/sec: 127.592\n",
      "INFO:tensorflow:loss = 0.6032799, step = 2900 (0.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.604\n",
      "INFO:tensorflow:loss = 0.5944968, step = 3000 (0.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.931\n",
      "INFO:tensorflow:loss = 0.6614905, step = 3100 (0.750 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.773\n",
      "INFO:tensorflow:loss = 0.654272, step = 3200 (0.893 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.07\n",
      "INFO:tensorflow:loss = 0.674116, step = 3300 (0.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.117\n",
      "INFO:tensorflow:loss = 0.59751934, step = 3400 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.806\n",
      "INFO:tensorflow:loss = 0.5893512, step = 3500 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.926\n",
      "INFO:tensorflow:loss = 0.6036701, step = 3600 (0.700 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3695 into ../models/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-07-21T15:54:31Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../models/model.ckpt-3695\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 1.75285s\n",
      "INFO:tensorflow:Finished evaluation at 2020-07-21-15:54:33\n",
      "INFO:tensorflow:Saving dict for global step 3695: accuracy = 0.57236844, accuracy_baseline = 0.57236844, auc = 0.78090185, auc_precision_recall = 0.73471856, average_loss = 0.6495791, global_step = 3695, label/mean = 0.4276316, loss = 0.6495791, precision = 0.0, prediction/mean = 0.30486476, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3695: ../models/model.ckpt-3695\n",
      "INFO:tensorflow:Loss for final step: 0.46817598.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "    myopt = tf.keras.optimizers.Ftrl(learning_rate = 0.01) # note the learning rate\n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "                        model_dir = output_dir, \n",
    "                        feature_columns = featcols.values(),\n",
    "                        hidden_units=[512, 128, 32],\n",
    "                        optimizer = myopt,\n",
    "                        dropout=0.2,\n",
    "                        n_classes=NUM_CLASSES)\n",
    "    \n",
    "    #estimator = tf.estimator.DNNClassifier(\n",
    "    #feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n",
    "    #hidden_units=[1024, 512, 256])\n",
    "  \n",
    "    #def my_auc(labels, predictions):\n",
    "    #    auc_metric = tf.keras.metrics.AUC(name=\"my_auc\")\n",
    "    #    auc_metric.update_state(y_true=labels, y_pred=predictions['logistic'])\n",
    "    #    return {'auc': auc_metric}\n",
    "\n",
    "    #estimator = tf.compat.v1.estimator.add_metrics(estimator, rmse)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn(df = traindf, batch_size = BATCH_SIZE),\n",
    "                                      max_steps = num_train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn(df = evaldf),\n",
    "                                        steps = None,                                        \n",
    "                                        start_delay_secs = 1, # start evaluating after N seconds\n",
    "                                        throttle_secs = 10  # evaluate every N seconds)\n",
    "                                     )\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "    \n",
    "# Run training    \n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate(OUTDIR, num_train_steps = (100 * len(traindf)) / BATCH_SIZE) "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
