{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T22:39:44.015641Z",
     "start_time": "2020-05-20T22:39:40.069585Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: jupyterlab-git 0.10.1 has requirement nbdime<2.0.0,>=1.1.0, but you'll have nbdime 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: apache-beam 2.22.0 has requirement httplib2<0.18.0,>=0.8, but you'll have httplib2 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: apache-beam 2.22.0 has requirement mock<3.0.0,>=1.0.1, but you'll have mock 4.0.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kaggle\n",
    "#!pip install google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json\n",
      "-rw------- 1 jupyter jupyter 73 Aug  6 15:38 /home/jupyter/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!ls -l ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T21:02:56.075438Z",
     "start_time": "2020-05-17T21:02:55.972412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access './../data': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "data_base_path = os.path.join(os.path.curdir, '../data')\n",
    "titanic_base_path = os.path.join(data_base_path, 'titanic')\n",
    "\n",
    "!ls $data_base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T22:37:37.832561Z",
     "start_time": "2020-05-20T22:37:37.826565Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_from_kaggle(dataset=\"titanic\", data_base_path=data_base_path):\n",
    "    data_path = os.path.join(os.path.curdir, f\"{data_base_path}/{dataset}\")\n",
    "    !kaggle competitions download -c $dataset --path $data_path --force\n",
    "    !unzip -o $data_path/titanic.zip -d $data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T22:38:01.443871Z",
     "start_time": "2020-05-20T22:38:00.330969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading titanic.zip to ././../data/titanic\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 26.3MB/s]\n",
      "Archive:  ././../data/titanic/titanic.zip\n",
      "  inflating: ././../data/titanic/gender_submission.csv  \n",
      "  inflating: ././../data/titanic/test.csv  \n",
      "  inflating: ././../data/titanic/train.csv  \n"
     ]
    }
   ],
   "source": [
    "load_data_from_kaggle(dataset=\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T23:48:52.908421Z",
     "start_time": "2020-05-23T23:48:52.904397Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_titanic_dateset(titanic_path=titanic_base_path):\n",
    "    gender_submission_csv_path = os.path.join(titanic_path, \"gender_submission.csv\")\n",
    "    train_csv_path = os.path.join(titanic_path, \"train.csv\")\n",
    "    test_csv_path = os.path.join(titanic_path, \"test.csv\")\n",
    "    return pd.read_csv(gender_submission_csv_path), pd.read_csv(train_csv_path), pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T23:48:55.753459Z",
     "start_time": "2020-05-23T23:48:55.702462Z"
    }
   },
   "outputs": [],
   "source": [
    "gender_sub_df, train_df, test_df = load_titanic_dateset(titanic_path=titanic_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T23:52:16.597837Z",
     "start_time": "2020-05-23T23:52:16.578833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.3</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male 22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female 38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female 26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female 35.0      1   \n",
       "4                           Allen, Mr. William Henry    male 35.0      0   \n",
       "\n",
       "   Parch            Ticket  Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2   NaN        S  \n",
       "1      0          PC 17599  71.3   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9   NaN        S  \n",
       "3      0            113803  53.1  C123        S  \n",
       "4      0            373450   8.1   NaN        S  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = None\n",
    "most_embarked_from = None\n",
    "train_fare_mean = None\n",
    "train_fare_std = None\n",
    "\n",
    "def drop_columns(df):\n",
    "    df.drop(\"Name\", axis=1, inplace=True)\n",
    "    df.drop(\"PassengerId\", axis=1, inplace=True)\n",
    "    df.drop(\"Ticket\", axis=1, inplace=True)\n",
    "    df.drop(\"Cabin\", axis=1, inplace=True)\n",
    "    df.drop(\"Sex_female\", axis=1, inplace=True)\n",
    "    df.drop('SibSp', axis=1, inplace=True)\n",
    "    df.drop('Parch', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    global median_age\n",
    "    global most_embarked_from\n",
    "    global train_fare_mean\n",
    "    global train_fare_std\n",
    "    \n",
    "    median_age = median_age or copy_df[\"Age\"].median(skipna=True)\n",
    "    most_embarked_from = most_embarked_from or copy_df[\"Embarked\"].value_counts().idxmax()\n",
    "    train_fare_mean = train_fare_mean or copy_df[\"Fare\"].mean(skipna=True)\n",
    "    train_fare_std = train_fare_std or copy_df[\"Fare\"].std(skipna=True)\n",
    "    \n",
    "    copy_df[\"Age\"].fillna(median_age, inplace=True)\n",
    "    copy_df[\"Embarked\"].fillna(most_embarked_from, inplace=True)\n",
    "    copy_df[\"Fare\"] = (copy_df[\"Fare\"] - train_fare_mean) / train_fare_std\n",
    "    \n",
    "    return copy_df\n",
    "    \n",
    "def create_categorical(df):\n",
    "    copy_df = pd.get_dummies(df, columns=[\"Pclass\", \"Embarked\", \"Title\", \"Sex\"])\n",
    "\n",
    "    return copy_df\n",
    "\n",
    "def add_engineered(df):\n",
    "    df['TravelAlone'] = np.where((df[\"SibSp\"] + df[\"Parch\"])>0, 0, 1)\n",
    "    df['TravelAlone'] = df['TravelAlone'].astype('uint8')\n",
    "    \n",
    "    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    \n",
    "    df['Title'] = df['Title'].replace(['Mlle','Mme','Ms'], 'Miss') # Mlle = Mademoiselle\n",
    "    return df\n",
    "\n",
    "def change_types(df):\n",
    "    df['TravelAlone'] = df['TravelAlone'].astype('uint8')\n",
    "    if 'Survived' in df.columns:\n",
    "        df['Survived'] = df['Survived'].astype('uint8')\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    copy_df = df.copy()\n",
    "    \n",
    "    copy_df = fill_missing_values(copy_df)\n",
    "    copy_df = add_engineered(copy_df)\n",
    "    copy_df = create_categorical(copy_df)\n",
    "    copy_df = drop_columns(copy_df)\n",
    "    copy_df = change_types(copy_df)\n",
    "    return copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_clean = clean_data(train_df)\n",
    "test_df_clean = clean_data(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>TravelAlone</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Rare</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Age  Fare  TravelAlone  Pclass_1  Pclass_2  Pclass_3  Embarked_C  \\\n",
       "0         0 22.0  -0.5            0         0         0         1           0   \n",
       "1         1 38.0   0.8            0         1         0         0           1   \n",
       "2         1 26.0  -0.5            1         0         0         1           0   \n",
       "3         1 35.0   0.4            0         1         0         0           0   \n",
       "4         0 35.0  -0.5            1         0         0         1           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  \\\n",
       "0           0           1             0           0         1          0   \n",
       "1           0           0             0           0         0          1   \n",
       "2           0           1             0           1         0          0   \n",
       "3           0           1             0           0         0          1   \n",
       "4           0           1             0           0         1          0   \n",
       "\n",
       "   Title_Rare  Sex_male  \n",
       "0           0         1  \n",
       "1           0         0  \n",
       "2           0         0  \n",
       "3           0         0  \n",
       "4           0         1  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_clean.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Survived      891 non-null    uint8  \n",
      " 1   Age           891 non-null    float64\n",
      " 2   Fare          891 non-null    float64\n",
      " 3   TravelAlone   891 non-null    uint8  \n",
      " 4   Pclass_1      891 non-null    uint8  \n",
      " 5   Pclass_2      891 non-null    uint8  \n",
      " 6   Pclass_3      891 non-null    uint8  \n",
      " 7   Embarked_C    891 non-null    uint8  \n",
      " 8   Embarked_Q    891 non-null    uint8  \n",
      " 9   Embarked_S    891 non-null    uint8  \n",
      " 10  Title_Master  891 non-null    uint8  \n",
      " 11  Title_Miss    891 non-null    uint8  \n",
      " 12  Title_Mr      891 non-null    uint8  \n",
      " 13  Title_Mrs     891 non-null    uint8  \n",
      " 14  Title_Rare    891 non-null    uint8  \n",
      " 15  Sex_male      891 non-null    uint8  \n",
      "dtypes: float64(2), uint8(14)\n",
      "memory usage: 26.2 KB\n"
     ]
    }
   ],
   "source": [
    "train_df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived\n",
      "Age\n",
      "Fare\n",
      "TravelAlone\n",
      "Pclass_1\n",
      "Pclass_2\n",
      "Pclass_3\n",
      "Embarked_C\n",
      "Embarked_Q\n",
      "Embarked_S\n",
      "Title_Master\n",
      "Title_Miss\n",
      "Title_Mr\n",
      "Title_Mrs\n",
      "Title_Rare\n",
      "Sex_male\n"
     ]
    }
   ],
   "source": [
    "for column in train_df_clean.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Age', 'Fare', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare', 'Sex_male'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featcols = {\n",
    "  colname : tf.feature_column.numeric_column(colname) \\\n",
    "    #for colname in 'Age,Fare,Pclass_1,Pclass_2,Pclass_3,Embarked_C,Embarked_Q,Embarked_S,Sex_male,TravelAlone'.split(',')\n",
    "    for colname in train_df_clean.columns if colname != 'Survived'\n",
    "}\n",
    "featcols.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and eval\n",
    "msk = np.random.rand(len(train_df_clean)) < 0.8\n",
    "traindf = train_df_clean[msk]\n",
    "evaldf = train_df_clean[~msk]\n",
    "\n",
    "BATCH_SIZE= 20\n",
    "OUTDIR = '../models'\n",
    "\n",
    "\n",
    "def make_input_fn(df, mode, batch_size = BATCH_SIZE):\n",
    "    global mean_train_fare\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        num_epochs = None # loop indefinetly\n",
    "        shuffle=True\n",
    "    else:\n",
    "        num_epochs = 1 # one run and it's over\n",
    "        shuffle=False\n",
    "    \n",
    "    return tf.compat.v1.estimator.inputs.pandas_input_fn(x = df[list(featcols.keys())],\n",
    "                                                y = df[\"Survived\"],\n",
    "                                                num_epochs = num_epochs,\n",
    "                                                batch_size = batch_size, \n",
    "                                                shuffle = shuffle)\n",
    "\n",
    "def train_input_fn(df, batch_size=BATCH_SIZE):\n",
    "    return make_input_fn(df, mode=tf.estimator.ModeKeys.TRAIN, batch_size=batch_size)\n",
    "\n",
    "def eval_input_fn(df):\n",
    "    return make_input_fn(df, mode=tf.estimator.ModeKeys.EVAL, batch_size=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../models', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ../models/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1177987, step = 0\n",
      "INFO:tensorflow:global_step/sec: 68.4994\n",
      "INFO:tensorflow:loss = 0.37103006, step = 100 (1.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.29\n",
      "INFO:tensorflow:loss = 0.46603346, step = 200 (0.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.581\n",
      "INFO:tensorflow:loss = 0.24868771, step = 300 (0.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.288\n",
      "INFO:tensorflow:loss = 0.49667636, step = 400 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7812\n",
      "INFO:tensorflow:loss = 0.35020572, step = 500 (1.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1921\n",
      "INFO:tensorflow:loss = 0.6473044, step = 600 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 142.132\n",
      "INFO:tensorflow:loss = 0.41313845, step = 700 (0.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.337\n",
      "INFO:tensorflow:loss = 0.23878774, step = 800 (0.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.243\n",
      "INFO:tensorflow:loss = 0.23375246, step = 900 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.97\n",
      "INFO:tensorflow:loss = 0.19047734, step = 1000 (0.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.15\n",
      "INFO:tensorflow:loss = 0.5461311, step = 1100 (0.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.527\n",
      "INFO:tensorflow:loss = 0.3779256, step = 1200 (0.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0894\n",
      "INFO:tensorflow:loss = 0.3810014, step = 1300 (1.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.325\n",
      "INFO:tensorflow:loss = 0.3601872, step = 1400 (0.882 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.866\n",
      "INFO:tensorflow:loss = 0.5909773, step = 1500 (0.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 121.57\n",
      "INFO:tensorflow:loss = 0.350368, step = 1600 (0.822 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.768\n",
      "INFO:tensorflow:loss = 0.51190794, step = 1700 (1.020 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5823\n",
      "INFO:tensorflow:loss = 0.35762, step = 1800 (1.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.512\n",
      "INFO:tensorflow:loss = 0.30261332, step = 1900 (0.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.7345\n",
      "INFO:tensorflow:loss = 0.5803145, step = 2000 (1.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.238\n",
      "INFO:tensorflow:loss = 0.43849063, step = 2100 (0.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 106.881\n",
      "INFO:tensorflow:loss = 0.31063142, step = 2200 (0.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.3752\n",
      "INFO:tensorflow:loss = 0.54632396, step = 2300 (1.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.947\n",
      "INFO:tensorflow:loss = 0.28095785, step = 2400 (0.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.8677\n",
      "INFO:tensorflow:loss = 0.4794014, step = 2500 (1.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.055\n",
      "INFO:tensorflow:loss = 0.30268568, step = 2600 (0.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 97.7572\n",
      "INFO:tensorflow:loss = 0.4401978, step = 2700 (1.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.447\n",
      "INFO:tensorflow:loss = 0.36789554, step = 2800 (0.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.485\n",
      "INFO:tensorflow:loss = 0.45163354, step = 2900 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.8788\n",
      "INFO:tensorflow:loss = 0.3283997, step = 3000 (1.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.9\n",
      "INFO:tensorflow:loss = 0.21447179, step = 3100 (0.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.91\n",
      "INFO:tensorflow:loss = 0.22475454, step = 3200 (0.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3553\n",
      "INFO:tensorflow:loss = 0.4742724, step = 3300 (1.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 108.93\n",
      "INFO:tensorflow:loss = 0.5903282, step = 3400 (0.920 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.808\n",
      "INFO:tensorflow:loss = 0.26746246, step = 3500 (0.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.969\n",
      "INFO:tensorflow:loss = 0.43712252, step = 3600 (0.846 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3615 into ../models/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-08-06T17:32:50Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../models/model.ckpt-3615\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 1.64609s\n",
      "INFO:tensorflow:Finished evaluation at 2020-08-06-17:32:52\n",
      "INFO:tensorflow:Saving dict for global step 3615: accuracy = 0.83928573, accuracy_baseline = 0.58928573, auc = 0.8829601, auc_precision_recall = 0.8843119, average_loss = 0.41082293, global_step = 3615, label/mean = 0.4107143, loss = 0.41082293, precision = 0.90384614, prediction/mean = 0.40558633, recall = 0.68115944\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3615: ../models/model.ckpt-3615\n",
      "INFO:tensorflow:Loss for final step: 0.3506985.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "    myopt = tf.keras.optimizers.Ftrl(learning_rate = 0.01, l1_regularization_strength=0.001) # note the learning rate\n",
    "    #ada_optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(learning_rate=0.1, l1_regularization_strength=0.001)\n",
    "    \n",
    "    adam_opt = tf.keras.optimizers.Adam(learning_rate=0.001 ) # note the learning rate\n",
    "\n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "                        model_dir = output_dir, \n",
    "                        feature_columns = featcols.values(),\n",
    "                        hidden_units=[1024, 256, 32],\n",
    "                        optimizer = adam_opt,\n",
    "                        dropout=0.2,\n",
    "                        n_classes=NUM_CLASSES)\n",
    "    \n",
    "    #estimator = tf.estimator.DNNClassifier(\n",
    "    #feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n",
    "    #hidden_units=[1024, 512, 256])\n",
    "  \n",
    "    #def my_auc(labels, predictions):\n",
    "    #    auc_metric = tf.keras.metrics.AUC(name=\"my_auc\")\n",
    "    #    auc_metric.update_state(y_true=labels, y_pred=predictions['logistic'])\n",
    "    #    return {'auc': auc_metric}\n",
    "\n",
    "    #estimator = tf.compat.v1.estimator.add_metrics(estimator, rmse)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn(df = traindf, batch_size = BATCH_SIZE),\n",
    "                                      max_steps = num_train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn(df = evaldf),\n",
    "                                        steps = None,                                        \n",
    "                                        start_delay_secs = 1, # start evaluating after N seconds\n",
    "                                        throttle_secs = 10  # evaluate every N seconds)\n",
    "                                     )\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "    \n",
    "# Run training    \n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "train_and_evaluate(OUTDIR, num_train_steps = (100 * len(traindf)) / BATCH_SIZE) "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
