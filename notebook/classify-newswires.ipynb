{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "print(tf.version.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring optimzers\n",
    "from tensorflow.keras import optimizers\n",
    "# custom losses and metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history_dict, what_to_plot, title, xlabel='Epochs', ylabel='Accuracy'):\n",
    "    # [(k1,l1),(k2,l2)]\n",
    "    \n",
    "    key1, plot_label_1 = what_to_plot[0]\n",
    "    key2, plot_label_2 = what_to_plot[1]\n",
    "        \n",
    "    epochs = range(1, len(history_dict[key1]) + 1)\n",
    "    plt.clf() # clear figure\n",
    "\n",
    "    plt.plot(epochs, history_dict[key1], 'bo', label=plot_label_1)\n",
    "    plt.plot(epochs, history_dict[key2], 'b', label=plot_label_2)\n",
    "    #plt.title('Training and validation accuracy')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def plot_loss_accuracy():\n",
    "#    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#    fig.suptitle('Horizontally stacked subplots')\n",
    "#    ax1.plot(x, y)\n",
    "#    ax2.plot(x, -y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
    "train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)\n",
    "\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)\n",
    "\n",
    "#one_hot_train_labels = to_categorical(train_labels)\n",
    "#one_hot_test_labels = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "#The best loss function to use in this case is categorical_crossentropy. It measures\n",
    "#the distance between two probability distributions: here, between the probability distribution\n",
    "#output by the network and the true distribution of the labels\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, \n",
    "                    batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_dict, what_to_plot=[('loss', 'Training loss'),('val_loss', 'Validation loss')], \n",
    "              title='Training and validation loss', xlabel='Epochs', ylabel='Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_dict, what_to_plot=[('accuracy', 'Training acc'),('val_accuracy', 'Validation acc')], \n",
    "              title='Training and validation accuracy', xlabel='Epochs', ylabel='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(history_dict, what_to_plot_loss=[('loss', 'Training loss'),('val_loss', 'Validation loss')],\n",
    "                          what_to_plot_acc=[('accuracy', 'Training acc'),('val_accuracy', 'Validation acc')]):\n",
    "    plot_history(history_dict, what_to_plot=what_to_plot_loss, \n",
    "              title='Training and validation loss', xlabel='Epochs', ylabel='Loss')\n",
    "    plot_history(history_dict, what_to_plot=what_to_plot_acc, \n",
    "              title='Training and validation accuracy', xlabel='Epochs', ylabel='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model overfits after 9 epochs\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=9, \n",
    "                    batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels, verbose=0)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(predictions[0].shape)\n",
    "\n",
    "predictions[0]\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned earlier that another way to encode the labels would be to cast them as\n",
    "an integer tensor, like this:\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "Then we would have to use sparse_categorical_crossentropy instead of categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create informational bottleneck :\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=128, validation_data=(x_val, y_val))\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "plot_loss_and_accuracy(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hyperparameters(parameters, partial_x_train=partial_x_train, partial_y_train=partial_y_train,\n",
    "                        x_val=x_val, y_val=y_val, x_test=x_test, y_test=one_hot_test_labels):\n",
    "    loss_function = parameters['loss_function']\n",
    "    dense_layer_count = parameters['dense_layer_count']\n",
    "    hidden_units = parameters['hidden_units']\n",
    "    activation_function = parameters['activation_function']\n",
    "    batch_size = parameters.get('batch_size') or 512\n",
    "    lr = parameters.get('lr') or 0.001\n",
    "    kernel_regularizer_param=parameters.get('kernel_regularizer') or None\n",
    "    #{'l1': 0.0010000000474974513, 'l2': 0.0}\n",
    "    kernel_regularizer = None\n",
    "    if kernel_regularizer_param is not None:\n",
    "        kernel_regularizer = regularizers.l1_l2(l1=kernel_regularizer_param['l1'] or 0.\n",
    "                                               ,l2=kernel_regularizer_param['l2'] or 0.)   \n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(hidden_units, kernel_regularizer=kernel_regularizer,\n",
    "                           activation=activation_function, input_shape=(10000,)))\n",
    "    for _ in range(dense_layer_count - 1):        \n",
    "        model.add(layers.Dense(hidden_units, kernel_regularizer=kernel_regularizer,\n",
    "                               activation=activation_function))\n",
    "        \n",
    "    #model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    # last output layer\n",
    "    model.add(layers.Dense(46, activation='softmax')) \n",
    "\n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=lr), \n",
    "                  loss=loss_function, metrics=['accuracy'])   \n",
    "    \n",
    "    history = model.fit(partial_x_train, partial_y_train,\n",
    "                   epochs=20, batch_size=batch_size, validation_data=(x_val, y_val))\n",
    "    \n",
    "    history_dict = history.history\n",
    "    results = model.evaluate(x_test, one_hot_test_labels, verbose=0)\n",
    "\n",
    "    print(results)\n",
    "    plot_loss_and_accuracy(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = regularizers.l1(0.001)\n",
    "print(regularizers.serialize(regularizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':1, \n",
    "              'hidden_units' : 16, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 16, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 32, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 64, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':3, \n",
    "              'hidden_units' : 64, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 128, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 64, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0.001, 'l2': 0.}}\n",
    "test_hyperparameters(parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 64, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0., 'l2': 0.001}}\n",
    "test_hyperparameters(parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 128, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0.001, 'l2': 0.}}\n",
    "test_hyperparameters(parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 256, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0.01, 'l2': 0.01}}\n",
    "test_hyperparameters(parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 256, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0., 'l2': 0.001}}\n",
    "test_hyperparameters(parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 256, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0., 'l2': 0.001}}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 64, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0., 'l2': 0.001}}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 128, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0.001, 'l2': 0.}}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 256, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0.01, 'l2': 0.01}}\n",
    "test_hyperparameters(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'loss_function' : 'categorical_crossentropy', 'dense_layer_count':2, \n",
    "              'hidden_units' : 256, 'activation_function': 'relu', 'batch_size': 128, 'lr': 0.001\n",
    "             , 'kernel_regularizer': {'l1': 0., 'l2': 0.001}}\n",
    "test_hyperparameters(parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}